{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numba\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "NUM_FEATS = 5\n",
    "\n",
    "class Net(object):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_layers, num_units):\n",
    "        \n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_units = num_units\n",
    "        \n",
    "\n",
    "        self.biases = []\n",
    "        self.weights = []\n",
    "        for i in range(num_layers):\n",
    "\n",
    "            if i==0:\n",
    "                # Input layer\n",
    "                self.weights.append(np.random.uniform(0, 1, size=(NUM_FEATS, self.num_units)))\n",
    "\n",
    "            else:\n",
    "                # Hidden layer\n",
    "                self.weights.append(np.random.uniform(0, 1, size=(self.num_units, self.num_units)))\n",
    "\n",
    "\n",
    "            self.biases.append(np.random.uniform(0, 1, size=(1, self.num_units)))\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        self.biases.append(np.random.uniform(0, 1, size=(1, 1)))\n",
    "        self.weights.append(np.random.uniform(0, 1, size=(self.num_units, 1)))\n",
    "\n",
    "    def __call__(self, X):\n",
    "       \n",
    "        a=X\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "        self.a_state = []\n",
    "        self.h_state = []\n",
    "        self.a_state.append(a)\n",
    "        for i in range(len(self.weights)):\n",
    "            h = np.dot(a, weights[i]) + biases[i]\n",
    "            if i<len(self.weights)-1:\n",
    "                self.h_state.append(h)\n",
    "                a = np.maximum(h, 0)\n",
    "                self.a_state.append(a)\n",
    "            else:\n",
    "                #self.h_state.append(h)\n",
    "                a=h\n",
    "                self.a_state.append(a)\n",
    "                \n",
    "        y_hat=a\n",
    "        return y_hat\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, X, y, lamda):\n",
    "        \n",
    "\n",
    "        del_W = []\n",
    "        del_b = []\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "        \n",
    "        y = np.reshape(y, (y.shape[0], 1))\n",
    "        n = y.shape[0]\n",
    "        \n",
    "        \n",
    "        a_state=self.a_state\n",
    "        h_state=self.h_state\n",
    "        \n",
    "        del1 = (1/n) * (a_state[-1] - y)\n",
    "        \n",
    "        delw = np.dot(a_state[-2].T, del1) + lamda * (weights[-1])\n",
    "        \n",
    "        delb = np.sum(del1, axis=0) + lamda * (biases[-1])\n",
    "        \n",
    "        del_W.append(delw)\n",
    "        del_b.append(delb)\n",
    "\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            del1=np.dot(del1,weights[-(i+1)].T)\n",
    "            delw=np.dot(a_state[-(i+3)].T,(del1*(h_state[-(i+1)]>0)))+lamda*(weights[-(i+2)])\n",
    "            delb=np.sum(del1*(h_state[-(i+1)]>0),axis=0) + lamda*(biases[-(i+2)])\n",
    "            del_W.insert(0,delw)\n",
    "            del_b.insert(0,delb)\n",
    "        \n",
    "\n",
    "        return del_W, del_b\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer(object):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "\n",
    "    def step(self, weights, biases, delta_weights, delta_biases):\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            weights[i] =weights[i] -self.learning_rate * delta_weights[i]\n",
    "            biases[i] =biases[i]- self.learning_rate * delta_biases[i]\n",
    "\n",
    "        return weights, biases\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_mse(y, y_hat):\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    y = np.reshape(y, (y.shape[0], 1))\n",
    "    mse = np.sum(np.square(y - y_hat))\n",
    "    mse=(1/m)*mse\n",
    "    return mse\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def loss_regularization(weights, biases):\n",
    "\n",
    "    l2_reg = 0\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        l2_reg += np.sum(np.square(weights[i])) + np.sum(np.square(biases[i]))\n",
    "    \n",
    "    return l2_reg\n",
    "    raise NotImplementedError\n",
    "\n",
    "def loss_fn(y, y_hat, weights, biases, lamda):\n",
    "    mse_ls=loss_mse(y, y_hat)\n",
    "    reg_ls=loss_regularization(weights, biases)\n",
    "    loss=mse_ls+lamda*reg_ls\n",
    "    return loss\n",
    "    raise NotImplementedError\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "    \n",
    "    rsme = np.sqrt(loss_mse(y, y_hat))\n",
    "    return rsme\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba.config.THREADING_LAYER='omp'\n",
    "@numba.jit(parallel=True)\n",
    "def batch(net, optimizer, lamda, batch_size,\n",
    "    train_input, train_target):\n",
    "    epoch_loss = 0.\n",
    "    m = train_input.shape[0]\n",
    "    for i in range(0, m, batch_size):\n",
    "            batch_input = train_input[i:i+batch_size]\n",
    "            batch_target = train_target[i:i+batch_size]\n",
    "            \n",
    "            pred = net(batch_input)\n",
    "            #print(pred)\n",
    "            # Compute gradients of loss w.r.t. weights and biases\n",
    "            dW, db = net.backward(batch_input, batch_target, lamda)\n",
    "            #print(dW)\n",
    "\n",
    "            # Get updated weights based on current weights and gradients\n",
    "            weights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)\n",
    "\n",
    "            # Update model's weights and biases\n",
    "            net.weights = weights_updated\n",
    "            net.biases = biases_updated\n",
    "            # Compute loss for the batch\n",
    "            batch_loss = loss_fn(batch_target, pred, net.weights, net.biases, lamda)\n",
    "            epoch_loss += batch_loss\n",
    "    return net.weights,net.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    Read the train, dev, and test datasets\n",
    "    '''\n",
    "    data=pd.read_csv('NSE-TATAGLOBAL11.csv')\n",
    "    X=data.drop(['Date','St Date','Close','Turnover (Lacs)','Total Trade Quantity'],axis=1)\n",
    "    Y=data['Close']\n",
    "    train_input_df,test_input_df,train_target_df,test_target_df=train_test_split(X,Y,test_size=0.1,random_state=44)\n",
    "    #type(train_input)\n",
    "    train_input=np.array(train_input_df)\n",
    "    train_target=np.array(train_target_df)\n",
    "    test_input=np.array(test_input_df)\n",
    "    test_target=np.array(test_target_df)\n",
    "#     print(train_input.shape)\n",
    "    return train_input, train_target,test_input,test_target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on Test Data: 16.333358473501605\n",
      "Time: 19.4927354\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "learning_rate =1e-10\n",
    "num_layers = 2\n",
    "num_units = 7\n",
    "lamda = 0.1 # Regularization Parameter\n",
    "\n",
    "train_input, train_target,test_input,test_target = read_data()\n",
    "net = Net(num_layers, num_units)\n",
    "optimizer = Optimizer(learning_rate)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import njit,  threading_layer, config, prange\n",
    "import timeit\n",
    "\n",
    "config.THREADING_LAYER = 'omp'\n",
    "config.NUMBA_NUM_THREADS=8\n",
    "start=timeit.default_timer()\n",
    "for e in range(max_epochs):\n",
    "    batch(net, optimizer, lamda, batch_size,train_input, train_target)\n",
    "end=timeit.default_timer()\n",
    "test_pred=net(test_input)\n",
    "test_rmse=rmse(test_target,test_pred)\n",
    "print('RMSE on Test Data:',test_rmse)\n",
    "print('Time:',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
